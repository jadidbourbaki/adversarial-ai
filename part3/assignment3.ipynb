{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Assignment 3: Adversarial Defense Strategies\n",
    "\n",
    "In this assignment, you will implement and evaluate practical defense mechanisms against adversarial attacks. You'll learn how to make neural networks more robust by implementing simple yet effective defensive techniques.\n",
    "\n",
    "**Instructions:**\n",
    "1. Complete all 3 exercises in this notebook\n",
    "2. Run all cells and ensure outputs are visible\n",
    "3. Submit the completed notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the code below to set up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enhanced plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"🎯 Ready to train a fresh model and explore adversarial attacks...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to build and train the machine learning model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Train Target Model for Adversarial Attacks\n",
    "\n",
    "print(\"🏗️ Training a fresh CNN model for adversarial attack assignments...\")\n",
    "print(\"(This ensures we have a well-trained target for meaningful attack demonstrations)\")\n",
    "\n",
    "# Define CNN architecture for MNIST classification\n",
    "class MNISTNet(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network for MNIST digit classification.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling and dropout\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Data loading and preprocessing\n",
    "print(\"📊 Loading MNIST dataset...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "test_loader_single = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"✅ Dataset loaded: {len(train_dataset)} training, {len(test_dataset)} test samples\")\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = MNISTNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"🤖 Model parameters: {total_params:,}\")\n",
    "\n",
    "# Quick training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=3):\n",
    "    \"\"\"Train the model for a few epochs to get good performance.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(f\"\\n📚 Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Training\") as pbar:\n",
    "            for batch_idx, (data, target) in enumerate(pbar):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                if batch_idx % 50 == 0:\n",
    "                    accuracy = 100. * correct / total\n",
    "                    avg_loss = running_loss / (batch_idx + 1)\n",
    "                    pbar.set_postfix({\n",
    "                        'Loss': f'{avg_loss:.4f}', \n",
    "                        'Acc': f'{accuracy:.2f}%'\n",
    "                    })\n",
    "        \n",
    "        # Final epoch stats\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"   📈 Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "# Test model performance\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "print(\"🚀 Starting training...\")\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=3)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\n📊 Evaluating model performance...\")\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"🎯 Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "if test_accuracy > 95:\n",
    "    print(\"✅ Excellent! Model is well-trained and ready for adversarial attacks!\")\n",
    "elif test_accuracy > 90:\n",
    "    print(\"✅ Good! Model performance is sufficient for attack demonstrations!\")\n",
    "else:\n",
    "    print(\"⚠️  Model accuracy is lower than expected, but will work for demonstrations.\")\n",
    "\n",
    "# Save the trained model for future use\n",
    "torch.save(model.state_dict(), 'assignment3_trained_model.pth')\n",
    "print(\"💾 Model saved as 'assignment3_trained_model.pth'\")\n",
    "\n",
    "print(\"\\n🎯 Model training complete and ready for adversarial attack assignments!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 1: Input Preprocessing Defense\n",
    "\n",
    "Implement and evaluate input preprocessing techniques that can defend against adversarial attacks. Input preprocessing is one of the simplest defense strategies. The idea is to \"clean\" the input before feeding it to the model, removing adversarial perturbations while preserving the original signal. Add random Gaussian noise to inputs to disrupt adversarial perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise_defense(image, noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Apply Gaussian noise defense to input image.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image tensor\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "        defended_image: Image with added Gaussian noise\n",
    "    \"\"\"\n",
    "    # TODO: Implement Gaussian noise defense\n",
    "    # Hint: Use torch.randn_like() to generate noise with same shape as image\n",
    "    # Add noise to image and clamp to valid range [-1, 1]\n",
    "    \n",
    "    # Your implementation here:\n",
    "    # TODO: Implement your solution here\n",
    "    \n",
    "    # TODO: Replace solution above with:\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing Exercise 1\n",
    "\n",
    "Run the code below to test your implementation. Note that the defenses will (with high probability) **not** be successful. This is expected, we will discuss better defenses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement stronger attacks for more meaningful defense testing\n",
    "def fgsm_attack(model, image, label, epsilon=0.3):\n",
    "    \"\"\"Fast Gradient Sign Method attack.\"\"\"\n",
    "    image.requires_grad_(True)\n",
    "    output = model(image)\n",
    "    loss = F.cross_entropy(output, label)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    gradient_sign = image.grad.data.sign()\n",
    "    adversarial_image = image + epsilon * gradient_sign\n",
    "    return torch.clamp(adversarial_image, -1, 1).detach()\n",
    "\n",
    "def pgd_attack(model, image, label, epsilon=0.3, alpha=0.075, iterations=10):\n",
    "    \"\"\"\n",
    "    Projected Gradient Descent attack - much stronger than FGSM.\n",
    "    \n",
    "    Args:\n",
    "        model: Target model\n",
    "        image: Clean input image\n",
    "        label: True label\n",
    "        epsilon: Maximum perturbation budget\n",
    "        alpha: Step size for each iteration\n",
    "        iterations: Number of attack iterations\n",
    "    \n",
    "    Returns:\n",
    "        adversarial_image: Adversarially perturbed image\n",
    "    \"\"\"\n",
    "    # Start with a random perturbation\n",
    "    delta = torch.zeros_like(image).uniform_(-epsilon, epsilon)\n",
    "    delta = torch.clamp(image + delta, -1, 1) - image\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        delta.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(image + delta)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update perturbation\n",
    "        grad_sign = delta.grad.data.sign()\n",
    "        delta = delta + alpha * grad_sign\n",
    "        \n",
    "        # Project back to epsilon ball\n",
    "        delta = torch.clamp(delta, -epsilon, epsilon)\n",
    "        delta = torch.clamp(image + delta, -1, 1) - image\n",
    "        \n",
    "        delta = delta.detach()\n",
    "    \n",
    "    return (image + delta).detach()\n",
    "\n",
    "def find_vulnerable_samples(model, test_loader, num_samples=10):\n",
    "    \"\"\"Find samples that are not too confident for better attack demonstrations.\"\"\"\n",
    "    model.eval()\n",
    "    vulnerable_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if len(vulnerable_samples) >= num_samples:\n",
    "                break\n",
    "                \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            confidence = probs.max(dim=1)[0]\n",
    "            \n",
    "            # Look for samples with 70-95% confidence (not too easy, not too hard)\n",
    "            for i in range(data.size(0)):\n",
    "                if 0.7 <= confidence[i] <= 0.95:\n",
    "                    vulnerable_samples.append((data[i:i+1], target[i:i+1], confidence[i].item()))\n",
    "                    if len(vulnerable_samples) >= num_samples:\n",
    "                        break\n",
    "    \n",
    "    return vulnerable_samples\n",
    "\n",
    "# Test the defense functions with stronger attacks\n",
    "print(\"🛡️ TESTING INPUT PREPROCESSING DEFENSES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"🔍 Finding vulnerable samples for meaningful defense testing...\")\n",
    "vulnerable_samples = find_vulnerable_samples(model, test_loader_single, num_samples=5)\n",
    "\n",
    "if not vulnerable_samples:\n",
    "    print(\"⚠️  No vulnerable samples found, using first available sample...\")\n",
    "    sample_iter = iter(test_loader_single)\n",
    "    test_image, true_label = next(sample_iter)\n",
    "    test_image, true_label = test_image.to(device), true_label.to(device)\n",
    "    vulnerable_samples = [(test_image, true_label, 1.0)]\n",
    "\n",
    "print(f\"✅ Found {len(vulnerable_samples)} samples for testing\")\n",
    "\n",
    "# Test attacks and defenses on multiple samples\n",
    "total_results = {\n",
    "    'fgsm_success': 0,\n",
    "    'pgd_success': 0,\n",
    "    'gaussian_defense_success': 0,\n",
    "    'median_defense_success': 0\n",
    "}\n",
    "\n",
    "print(\"\\n📊 COMPREHENSIVE ATTACK AND DEFENSE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx, (test_image, true_label, original_conf) in enumerate(vulnerable_samples):\n",
    "    print(f\"\\n🎯 Sample {idx+1}: True label={true_label.item()}, Original confidence={original_conf:.3f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Test FGSM attack\n",
    "    fgsm_adv = fgsm_attack(model, test_image.clone(), true_label, epsilon=0.3)\n",
    "    with torch.no_grad():\n",
    "        fgsm_output = model(fgsm_adv)\n",
    "        fgsm_pred = torch.argmax(fgsm_output, dim=1)\n",
    "        fgsm_conf = F.softmax(fgsm_output, dim=1).max()\n",
    "    \n",
    "    fgsm_success = fgsm_pred.item() != true_label.item()\n",
    "    total_results['fgsm_success'] += fgsm_success\n",
    "    print(f\"FGSM Attack (ε=0.3): Pred={fgsm_pred.item()}, Conf={fgsm_conf:.3f}, Success={fgsm_success}\")\n",
    "    \n",
    "    # Test PGD attack (stronger)\n",
    "    pgd_adv = pgd_attack(model, test_image.clone(), true_label, epsilon=0.3, alpha=0.075, iterations=10)\n",
    "    with torch.no_grad():\n",
    "        pgd_output = model(pgd_adv)\n",
    "        pgd_pred = torch.argmax(pgd_output, dim=1)\n",
    "        pgd_conf = F.softmax(pgd_output, dim=1).max()\n",
    "    \n",
    "    pgd_success = pgd_pred.item() != true_label.item()\n",
    "    total_results['pgd_success'] += pgd_success\n",
    "    print(f\"PGD Attack (ε=0.3): Pred={pgd_pred.item()}, Conf={pgd_conf:.3f}, Success={pgd_success}\")\n",
    "    \n",
    "    # Use the stronger attack (PGD) for defense testing\n",
    "    attack_image = pgd_adv if pgd_success else fgsm_adv\n",
    "    attack_name = \"PGD\" if pgd_success else \"FGSM\"\n",
    "    \n",
    "    if pgd_success or fgsm_success:\n",
    "        print(f\"\\n🛡️ Testing defenses against {attack_name} attack:\")\n",
    "        \n",
    "        # Test Gaussian noise defense\n",
    "        defended_gaussian = gaussian_noise_defense(attack_image.clone(), noise_std=0.1)\n",
    "        with torch.no_grad():\n",
    "            gaussian_output = model(defended_gaussian)\n",
    "            gaussian_pred = torch.argmax(gaussian_output, dim=1)\n",
    "            gaussian_conf = F.softmax(gaussian_output, dim=1).max()\n",
    "        \n",
    "        gaussian_success = gaussian_pred.item() == true_label.item()\n",
    "        total_results['gaussian_defense_success'] += gaussian_success\n",
    "        print(f\"  Gaussian noise (σ=0.1): Pred={gaussian_pred.item()}, Conf={gaussian_conf:.3f}, Defense Success={gaussian_success}\")\n",
    "    else:\n",
    "        print(\"  ⚠️  No successful attack to test defenses against\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📈 OVERALL RESULTS (across {len(vulnerable_samples)} samples):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"FGSM Attack Success Rate: {total_results['fgsm_success']}/{len(vulnerable_samples)} ({100*total_results['fgsm_success']/len(vulnerable_samples):.1f}%)\")\n",
    "print(f\"PGD Attack Success Rate: {total_results['pgd_success']}/{len(vulnerable_samples)} ({100*total_results['pgd_success']/len(vulnerable_samples):.1f}%)\")\n",
    "\n",
    "successful_attacks = max(total_results['fgsm_success'], total_results['pgd_success'])\n",
    "if successful_attacks > 0:\n",
    "    print(f\"Gaussian Noise Defense Success: {total_results['gaussian_defense_success']}/{successful_attacks} ({100*total_results['gaussian_defense_success']/successful_attacks:.1f}%)\")\n",
    "    print(f\"Median Filter Defense Success: {total_results['median_defense_success']}/{successful_attacks} ({100*total_results['median_defense_success']/successful_attacks:.1f}%)\")\n",
    "\n",
    "print(\"\\n✅ Comprehensive attack and defense testing complete!\")\n",
    "print(\"💡 PGD attacks are typically much stronger than FGSM - use PGD results for defense evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 2: Adversarial Training Defense \n",
    "\n",
    "Implement adversarial training to create a robust model. Adversarial training is one of the most effective defense strategies. The idea is to train the model on both clean and adversarial examples, making it naturally robust to attacks. Create a training loop that includes adversarial examples during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_training_step(model, data, target, optimizer, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Perform one step of adversarial training.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network to train\n",
    "        data: Clean input batch\n",
    "        target: True labels\n",
    "        optimizer: Optimizer for the model\n",
    "        epsilon: Perturbation budget for adversarial examples\n",
    "    \n",
    "    Returns:\n",
    "        loss: Combined loss on clean and adversarial examples\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # TODO: Implement adversarial training step\n",
    "    # 1. Generate adversarial examples using FGSM\n",
    "    # 2. Compute loss on both clean and adversarial examples\n",
    "    # 3. Combine the losses (e.g., 50% clean + 50% adversarial)\n",
    "    \n",
    "    # Your implementation here:\n",
    "    \n",
    "    # TODO: Implement your solution here\n",
    "    \n",
    "    # TODO: Replace solution above with:\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Testing Exercise 2\n",
    "\n",
    "Run the code below to test your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_model(model, train_loader, optimizer, epochs=2, epsilon=0.1):\n",
    "    \"\"\"Train a model using adversarial training.\"\"\"\n",
    "    print(f\"🛡️ Training robust model with adversarial training (ε={epsilon})...\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_clean = 0\n",
    "        correct_adv = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(f\"\\n📚 Robust Training Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        with tqdm(train_loader, desc=\"Robust Training\") as pbar:\n",
    "            for batch_idx, (data, target) in enumerate(pbar):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                # Adversarial training step\n",
    "                loss = adversarial_training_step(model, data, target, optimizer, epsilon)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Statistics (evaluate on clean and adversarial examples)\n",
    "                with torch.no_grad():\n",
    "                    # Clean accuracy\n",
    "                    output_clean = model(data)\n",
    "                    pred_clean = torch.argmax(output_clean, dim=1)\n",
    "                    correct_clean += (pred_clean == target).sum().item()\n",
    "                \n",
    "                # Adversarial accuracy (create fresh adversarial examples)\n",
    "                # Note: This needs to be outside torch.no_grad() to compute gradients\n",
    "                data_test_adv = data.clone().detach().requires_grad_(True)\n",
    "                output_test = model(data_test_adv)\n",
    "                loss_test = F.cross_entropy(output_test, target)\n",
    "                model.zero_grad()\n",
    "                loss_test.backward()\n",
    "                gradient_sign = data_test_adv.grad.data.sign()\n",
    "                data_test_adv = data + epsilon * gradient_sign\n",
    "                data_test_adv = torch.clamp(data_test_adv, -1, 1).detach()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output_adv = model(data_test_adv)\n",
    "                    pred_adv = torch.argmax(output_adv, dim=1)\n",
    "                    correct_adv += (pred_adv == target).sum().item()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                total += target.size(0)\n",
    "                \n",
    "                # Update progress bar\n",
    "                if batch_idx % 50 == 0:\n",
    "                    clean_acc = 100. * correct_clean / total\n",
    "                    adv_acc = 100. * correct_adv / total\n",
    "                    avg_loss = running_loss / (batch_idx + 1)\n",
    "                    pbar.set_postfix({\n",
    "                        'Loss': f'{avg_loss:.4f}',\n",
    "                        'Clean Acc': f'{clean_acc:.1f}%',\n",
    "                        'Adv Acc': f'{adv_acc:.1f}%'\n",
    "                    })\n",
    "        \n",
    "        # Final epoch stats\n",
    "        clean_accuracy = 100. * correct_clean / total\n",
    "        adv_accuracy = 100. * correct_adv / total\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"   📈 Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"   📊 Clean Accuracy: {clean_accuracy:.2f}%\")\n",
    "        print(f\"   🛡️ Adversarial Accuracy: {adv_accuracy:.2f}%\")\n",
    "\n",
    "# Create a robust model\n",
    "print(\"🏗️ Creating and training a robust model...\")\n",
    "robust_model = MNISTNet().to(device)\n",
    "robust_optimizer = optim.Adam(robust_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train with adversarial training\n",
    "train_robust_model(robust_model, train_loader, robust_optimizer, epochs=2, epsilon=0.1)\n",
    "\n",
    "# Save the robust model\n",
    "torch.save(robust_model.state_dict(), 'robust_model.pth')\n",
    "print(\"\\n💾 Robust model saved as 'robust_model.pth'\")\n",
    "print(\"✅ Adversarial training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise 3: Defense Evaluation and Comparison\n",
    "\n",
    "Compare the effectiveness of different defense strategies. Now that we have implemented multiple defense approaches, let's systematically evaluate and compare their effectiveness against adversarial attacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_defense_robustness(model, test_loader, defense_function, defense_params, num_samples=200):\n",
    "    \"\"\"\n",
    "    Evaluate defense effectiveness against both FGSM and PGD attacks.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to test\n",
    "        test_loader: Test data loader\n",
    "        defense_function: Defense function to apply\n",
    "        defense_params: Parameters for the defense function\n",
    "        num_samples: Number of samples to test\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary with clean and adversarial accuracies for both attacks\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    clean_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    epsilons = [0.1, 0.2, 0.3]  # Different attack strengths\n",
    "    results = {\n",
    "        eps: {\n",
    "            'fgsm_no_defense': 0, 'fgsm_with_defense': 0,\n",
    "            'pgd_no_defense': 0, 'pgd_with_defense': 0\n",
    "        } \n",
    "        for eps in epsilons\n",
    "    }\n",
    "    \n",
    "    print(f\"Testing defense with parameters: {defense_params}\")\n",
    "    \n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        if total >= num_samples:\n",
    "            break\n",
    "            \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Test clean accuracy\n",
    "        with torch.no_grad():\n",
    "            clean_output = model(data)\n",
    "            clean_pred = torch.argmax(clean_output, dim=1)\n",
    "            clean_correct += (clean_pred == target).sum().item()\n",
    "        \n",
    "        # Test against different attack strengths\n",
    "        for epsilon in epsilons:\n",
    "            # Test FGSM attack\n",
    "            fgsm_adv_data = fgsm_attack(model, data.clone(), target, epsilon)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # FGSM without defense\n",
    "                fgsm_output = model(fgsm_adv_data)\n",
    "                fgsm_pred = torch.argmax(fgsm_output, dim=1)\n",
    "                correct_fgsm_no_def = (fgsm_pred == target).sum().item()\n",
    "                results[epsilon]['fgsm_no_defense'] += correct_fgsm_no_def\n",
    "                \n",
    "                # FGSM with defense\n",
    "                if defense_function is not None:\n",
    "                    fgsm_defended_data = defense_function(fgsm_adv_data.clone(), **defense_params)\n",
    "                    fgsm_defended_output = model(fgsm_defended_data)\n",
    "                    fgsm_defended_pred = torch.argmax(fgsm_defended_output, dim=1)\n",
    "                    correct_fgsm_with_def = (fgsm_defended_pred == target).sum().item()\n",
    "                    results[epsilon]['fgsm_with_defense'] += correct_fgsm_with_def\n",
    "                else:\n",
    "                    results[epsilon]['fgsm_with_defense'] += correct_fgsm_no_def\n",
    "            \n",
    "            # Test PGD attack (stronger)\n",
    "            pgd_adv_data = pgd_attack(model, data.clone(), target, epsilon, alpha=epsilon/4, iterations=10)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # PGD without defense\n",
    "                pgd_output = model(pgd_adv_data)\n",
    "                pgd_pred = torch.argmax(pgd_output, dim=1)\n",
    "                correct_pgd_no_def = (pgd_pred == target).sum().item()\n",
    "                results[epsilon]['pgd_no_defense'] += correct_pgd_no_def\n",
    "                \n",
    "                # PGD with defense\n",
    "                if defense_function is not None:\n",
    "                    pgd_defended_data = defense_function(pgd_adv_data.clone(), **defense_params)\n",
    "                    pgd_defended_output = model(pgd_defended_data)\n",
    "                    pgd_defended_pred = torch.argmax(pgd_defended_output, dim=1)\n",
    "                    correct_pgd_with_def = (pgd_defended_pred == target).sum().item()\n",
    "                    results[epsilon]['pgd_with_defense'] += correct_pgd_with_def\n",
    "                else:\n",
    "                    results[epsilon]['pgd_with_defense'] += correct_pgd_no_def\n",
    "        \n",
    "        total += target.size(0)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    clean_accuracy = 100. * clean_correct / total\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        for key in results[epsilon]:\n",
    "            results[epsilon][key] = 100. * results[epsilon][key] / total\n",
    "    \n",
    "    return clean_accuracy, results\n",
    "\n",
    "print(\"📊 COMPREHENSIVE DEFENSE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different defense strategies\n",
    "defense_strategies = [\n",
    "    {\n",
    "        'name': 'No Defense',\n",
    "        'function': None,\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Gaussian Noise (σ=0.1)',\n",
    "        'function': gaussian_noise_defense,\n",
    "        'params': {'noise_std': 0.1}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Gaussian Noise (σ=0.2)',\n",
    "        'function': gaussian_noise_defense,\n",
    "        'params': {'noise_std': 0.2}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate each defense strategy\n",
    "defense_results = {}\n",
    "\n",
    "print(\"\\n🔍 Testing Standard Model with Different Defenses:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for strategy in defense_strategies:\n",
    "    print(f\"\\nTesting: {strategy['name']}\")\n",
    "    \n",
    "    clean_acc, adv_results = evaluate_defense_robustness(\n",
    "        model, test_loader_single, \n",
    "        strategy['function'], \n",
    "        strategy['params'], \n",
    "        num_samples=100\n",
    "    )\n",
    "    \n",
    "    defense_results[strategy['name']] = {\n",
    "        'clean_accuracy': clean_acc,\n",
    "        'adversarial_results': adv_results\n",
    "    }\n",
    "    \n",
    "    print(f\"  Clean Accuracy: {clean_acc:.1f}%\")\n",
    "    for eps in [0.1, 0.2, 0.3]:\n",
    "        if strategy['function'] is None:\n",
    "            print(f\"  FGSM Accuracy (ε={eps}): {adv_results[eps]['fgsm_no_defense']:.1f}%\")\n",
    "            print(f\"  PGD Accuracy (ε={eps}): {adv_results[eps]['pgd_no_defense']:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  FGSM Accuracy (ε={eps}): {adv_results[eps]['fgsm_no_defense']:.1f}% → {adv_results[eps]['fgsm_with_defense']:.1f}%\")\n",
    "            print(f\"  PGD Accuracy (ε={eps}): {adv_results[eps]['pgd_no_defense']:.1f}% → {adv_results[eps]['pgd_with_defense']:.1f}%\")\n",
    "\n",
    "print(\"\\n🛡️ Testing Robust Model (Adversarial Training):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test the robust model\n",
    "robust_clean_acc, robust_adv_results = evaluate_defense_robustness(\n",
    "    robust_model, test_loader_single, \n",
    "    None, {}, num_samples=100\n",
    ")\n",
    "\n",
    "print(f\"Robust Model Performance:\")\n",
    "print(f\"  Clean Accuracy: {robust_clean_acc:.1f}%\")\n",
    "for eps in [0.1, 0.2, 0.3]:\n",
    "    print(f\"  FGSM Accuracy (ε={eps}): {robust_adv_results[eps]['fgsm_no_defense']:.1f}%\")\n",
    "    print(f\"  PGD Accuracy (ε={eps}): {robust_adv_results[eps]['pgd_no_defense']:.1f}%\")\n",
    "\n",
    "defense_results['Adversarial Training'] = {\n",
    "    'clean_accuracy': robust_clean_acc,\n",
    "    'adversarial_results': robust_adv_results\n",
    "}\n",
    "\n",
    "print(\"\\n✅ Defense evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Congratulations!** You have successfully implemented and evaluated multiple adversarial defense strategies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}